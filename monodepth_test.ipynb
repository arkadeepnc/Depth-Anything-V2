{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "encoder = 'vitl' # or 'vits', 'vitb', 'vitg'\n",
    "\n",
    "model = DepthAnythingV2(**model_configs[encoder])\n",
    "model.load_state_dict(torch.load(f'checkpoints/depth_anything_v2_{encoder}.pth', map_location='cuda:0'))\n",
    "model.cuda()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_fname = '/home/atkesonlab/multLightWorkspace/data/lego_toy/images/left_1.png'\n",
    "depth_fname = '/home/atkesonlab/multLightWorkspace/data/lego_toy/depths/left_1_depth.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img = cv2.imread(image_fname)\n",
    "# raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)/255.0\n",
    "# plt.imshow(raw_img)\n",
    "depth = model.infer_image(raw_img) # HxW raw depth map in numpy\n",
    "# plt.imshow(depth)\n",
    "# plt.imsave('./depth.png', depth)\n",
    "depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "from kornia.geometry import depth_to_3d, depth_to_normals\n",
    "cam_mtx = np.array([[       1553.9663302771853,      0.0,       339.47357153102087    ],\n",
    "                    [      0.0,      1556.191376668082,      288.44960314558057    ],\n",
    "                    [      0.0,       0.0,       1.0    ]  ])\n",
    "points = depth_to_3d (torch.from_numpy(depth[None, None, ...]),camera_matrix= torch.from_numpy(cam_mtx[ None, ...]),normalize_points= False).squeeze()\n",
    "x, y, z = points[0,...].flatten(), points[1,...].flatten(), points[2,...].flatten()\n",
    "\n",
    "normals = depth_to_normals( depth = torch.from_numpy(depth[None, None, ...]),camera_matrix= torch.from_numpy(cam_mtx[ None, ...])).squeeze()\n",
    "nx, ny, nz = normals[0,...].flatten(), normals[1,...].flatten(), normals[2,...].flatten()\n",
    "# plt.imshow((normals.transpose(2,0).cpu().numpy()+1)/2)\n",
    "# plt.imsave('./normals.png', (normals.permute(1,2,0).cpu().numpy()+1)/2)\n",
    "\n",
    "\n",
    "raw_img = cv2.imread(image_fname)\n",
    "raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)/255.0\n",
    "\n",
    "r,g,b = raw_img[...,0].flatten(), raw_img[...,1].flatten(), raw_img[...,2].flatten()\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pts = torch.stack([x,y,z], dim=0).detach().cpu().numpy()\n",
    "normals = torch.stack([nx,ny,nz], dim=0).detach().cpu().numpy()\n",
    "pcd.points = o3d.utility.Vector3dVector(pts.T)\n",
    "pcd.normals = o3d.utility.Vector3dVector(normals.T)\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.vstack([r,g,b]).T)\n",
    "\n",
    "o3d.io.write_point_cloud('./points.ply', pcd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import felzenszwalb, quickshift \n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "# segments = felzenszwalb(raw_img, scale = 0.1, sigma= 0.1, min_size= 200)\n",
    "segments = quickshift(raw_img, kernel_size=11, max_dist=6, ratio=0.5)\n",
    "plt.imshow(mark_boundaries(raw_img, segments))\n",
    "plt.show()\n",
    "num_segments = np.max(segments)\n",
    "# print(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_depth = np.load(depth_fname)\n",
    "# plt.imsave('./depth.png', true_depth)\n",
    "scaled_depth = np.zeros_like(true_depth)\n",
    "scale_all= []\n",
    "shift_all = []\n",
    "weights_all = []\n",
    "\n",
    "def compute_scale_and_shift(prediction, target, mask):\n",
    "    # system matrix: A = [[a_00, a_01], [a_10, a_11]]\n",
    "    # print(mask.shape, target.shape, prediction.shape,'<---??')\n",
    "    a_00 = torch.sum(mask * prediction * prediction, (1, 2))\n",
    "    a_01 = torch.sum(mask * prediction, (1, 2))\n",
    "    a_11 = torch.sum(mask, (1, 2))\n",
    "\n",
    "    # right hand side: b = [b_0, b_1]\n",
    "    b_0 = torch.sum(mask * prediction * target, (1, 2))\n",
    "    b_1 = torch.sum(mask * target, (1, 2))\n",
    "\n",
    "    # solution: x = A^-1 . b = [[a_11, -a_01], [-a_10, a_00]] / (a_00 * a_11 - a_01 * a_10) . b\n",
    "    x_0 = torch.zeros_like(b_0)\n",
    "    x_1 = torch.zeros_like(b_1)\n",
    "\n",
    "    det = a_00 * a_11 - a_01 * a_01\n",
    "    valid = det.nonzero()\n",
    "\n",
    "    x_0[valid] = (a_11[valid] * b_0[valid] - a_01[valid] * b_1[valid]) / det[valid]\n",
    "    x_1[valid] = (-a_01[valid] * b_0[valid] + a_00[valid] * b_1[valid]) / det[valid]\n",
    "\n",
    "    return x_0, x_1\n",
    "\n",
    "true_depth_t = torch.tensor(true_depth[None,...], dtype=torch.float32, device='cuda:0', requires_grad=False)\n",
    "pred_depth_t = torch.tensor(depth[None,...], dtype=torch.float32, device='cuda:0', requires_grad=False)\n",
    "out_depth_t = torch.zeros_like(true_depth_t) \n",
    "# out_depth = np.zeros_like(true_depth) \n",
    "for id in range(num_segments):\n",
    "    mask = (segments == id)\n",
    "    mask_t = torch.tensor(mask[None,...], dtype=torch.float32, device='cuda:0', requires_grad=False)\n",
    "    scale, shift = compute_scale_and_shift(pred_depth_t, true_depth_t, mask_t)\n",
    "    # out_depth_t = scale.view(-1, 1, 1) * pred_depth_t + shift.view(-1, 1, 1)\n",
    "    # out_depth_t += scale.view(-1, 1, 1) * pred_depth_t * mask_t + shift.view(-1, 1, 1)\n",
    "    scale_all.append(scale.cpu().numpy())\n",
    "    shift_all.append(shift.cpu().numpy())\n",
    "    weights_all.append(mask.sum())\n",
    "\n",
    "# out_depth = out_depth_t.squeeze().detach().cpu().numpy()\n",
    "\n",
    "# plt.imshow(out_depth)\n",
    "# plt.show()\n",
    "\n",
    "# print( out_depth.min(), out_depth.max())\n",
    "\n",
    "# plt.plot(scale_all, label = 'scale')\n",
    "# # plt.plot(shift_all, label = 'shift')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "from scipy.stats import mode\n",
    "# mode_scale = np.median(np.array(scale_all))\n",
    "# mode_shift = np.median(np.array(shift_all))\n",
    "mode_scale = np.average(np.array(scale_all).squeeze(), weights = np.array(weights_all))\n",
    "mode_shift = np.average(np.array(shift_all).squeeze(), weights = np.array(weights_all))\n",
    "\n",
    "print(f\"mode scele {mode_scale} mode shift {mode_shift}\")\n",
    "print(scale_all[-1], shift_all[-1])\n",
    "out_depth = depth * mode_scale + shift_all[-1]\n",
    "\n",
    "plt.imshow(out_depth)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "points = depth_to_3d (torch.from_numpy(out_depth[None, None, ...]),camera_matrix= torch.from_numpy(cam_mtx[ None, ...]),normalize_points= False).squeeze()\n",
    "x, y, z = points[0,...].flatten(), points[1,...].flatten(), points[2,...].flatten()\n",
    "\n",
    "normals = depth_to_normals( depth = torch.from_numpy(out_depth[None, None, ...]),camera_matrix= torch.from_numpy(cam_mtx[ None, ...])).squeeze()\n",
    "nx, ny, nz = normals[0,...].flatten(), normals[1,...].flatten(), normals[2,...].flatten()\n",
    "# plt.imshow((normals.transpose(2,0).cpu().numpy()+1)/2)\n",
    "# plt.imsave('./normals.png', (normals.permute(1,2,0).cpu().numpy()+1)/2)\n",
    "\n",
    "\n",
    "raw_img = cv2.imread(image_fname)\n",
    "raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)/255.0\n",
    "\n",
    "r,g,b = raw_img[...,0].flatten(), raw_img[...,1].flatten(), raw_img[...,2].flatten()\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pts = torch.stack([x,y,z], dim=0).detach().cpu().numpy()\n",
    "normals = torch.stack([nx,ny,nz], dim=0).detach().cpu().numpy()\n",
    "pcd.points = o3d.utility.Vector3dVector(pts.T)\n",
    "pcd.normals = o3d.utility.Vector3dVector(normals.T)\n",
    "pcd.colors = o3d.utility.Vector3dVector(np.vstack([r,g,b]).T)\n",
    "\n",
    "o3d.io.write_point_cloud('./points_upgraded.ply', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"mask-generation\", model=\"facebook/sam-vit-huge\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(raw_img[:,:,::-1])\n",
    "from PIL import Image\n",
    "import gc\n",
    "raw_img = '/home/atkesonlab/multLightWorkspace/data/lego_toy/images/left_1.png'\n",
    "raw_img = Image.open(image_fname).convert(\"RGB\")\n",
    "outputs = generator(raw_img, points_per_batch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    del mask\n",
    "    gc.collect()\n",
    "\n",
    "def show_masks_on_image(raw_image, masks):\n",
    "  plt.imshow(np.array(raw_image))\n",
    "  ax = plt.gca()\n",
    "  ax.set_autoscale_on(False)\n",
    "  for mask in masks:\n",
    "      show_mask(mask, ax=ax, random_color=True)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "  del mask\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = outputs[\"masks\"]\n",
    "scores = outputs[\"scores\"]\n",
    "print(scores)\n",
    "# show_masks_on_image(raw_img, masks)\n",
    "for mask, score in zip(masks, scores):\n",
    "    plt.imshow(mask)\n",
    "    plt.title(f\"score : {score}\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
